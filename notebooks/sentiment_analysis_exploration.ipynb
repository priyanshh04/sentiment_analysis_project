{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon Musical Instruments Reviews - Sentiment Analysis\n",
    "\n",
    "This notebook provides an interactive exploration of the sentiment analysis pipeline for Amazon Musical Instruments Reviews dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Add src to path\n",
    "sys.path.append(\"../src\")\n",
    "\n",
    "# Import custom modules\n",
    "from utils.data_loader import DataLoader, get_data_summary\n",
    "from preprocessing.feature_engineering import FeatureEngineer\n",
    "from preprocessing.text_preprocessing import TextPreprocessor\n",
    "from models.sentiment_models import SentimentModelTrainer\n",
    "from evaluation.model_evaluation import ModelEvaluator\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Disable warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"Setup completed successfully!\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "data_path = \"../data\"\n",
    "loader = DataLoader(data_path)\n",
    "\n",
    "# Try to load processed data first, then raw data\n",
    "try:\n",
    "    df = loader.load_processed_data()\n",
    "    print(\"Loaded processed data\")\n",
    "except:\n",
    "    df = loader.load_raw_data()\n",
    "    print(\"Loaded raw data\")\n",
    "\n",
    "print(f\"Dataset shape: {df.shape}\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get comprehensive data information\n",
    "info = loader.get_data_info()\n",
    "\n",
    "print(\"DATASET INFORMATION:\")\n",
    "print(\"=\" * 40)\n",
    "for key, value in info.items():\n",
    "    if key != 'data_types':\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize rating distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Rating distribution\n",
    "plt.subplot(1, 2, 1)\n",
    "rating_counts = df['overall'].value_counts().sort_index()\n",
    "plt.bar(rating_counts.index, rating_counts.values, color='skyblue')\n",
    "plt.xlabel('Rating (Stars)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Rating Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add percentage labels\n",
    "total = len(df)\n",
    "for i, v in enumerate(rating_counts.values):\n",
    "    plt.text(rating_counts.index[i], v + 50, f'{v/total*100:.1f}%', \n",
    "             ha='center', va='bottom')\n",
    "\n",
    "# Review length distribution\n",
    "plt.subplot(1, 2, 2)\n",
    "review_lengths = df['reviewText'].fillna('').apply(len)\n",
    "plt.hist(review_lengths, bins=50, color='lightcoral', alpha=0.7)\n",
    "plt.xlabel('Review Length (characters)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Review Length Distribution')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply feature engineering if not already done\n",
    "if 'sentiment_binary' not in df.columns:\n",
    "    print(\"Applying feature engineering...\")\n",
    "    feature_engineer = FeatureEngineer()\n",
    "    df = feature_engineer.fit_transform(df)\n",
    "    \n",
    "    print(f\"Added features: {feature_engineer.get_feature_names()}\")\n",
    "else:\n",
    "    print(\"Features already engineered\")\n",
    "\n",
    "# Show target variable distribution\n",
    "print(\"\\nTarget Variable Distribution:\")\n",
    "print(df['sentiment_binary'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize engineered features\n",
    "fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "features_to_plot = [\n",
    "    'review_length', 'review_word_count', 'helpful_votes',\n",
    "    'exclamation_count', 'question_count', 'capital_letter_ratio'\n",
    "]\n",
    "\n",
    "for i, feature in enumerate(features_to_plot):\n",
    "    if feature in df.columns:\n",
    "        axes[i].hist(df[feature], bins=30, alpha=0.7)\n",
    "        axes[i].set_title(f'{feature.replace(\"_\", \" \").title()}')\n",
    "        axes[i].set_xlabel(feature)\n",
    "        axes[i].set_ylabel('Frequency')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing example\n",
    "sample_text = df['reviewText'].dropna().iloc[10]\n",
    "\n",
    "print(\"ORIGINAL TEXT:\")\n",
    "print(\"-\" * 50)\n",
    "print(sample_text[:300] + \"...\")\n",
    "\n",
    "# Initialize preprocessor\n",
    "preprocessor = TextPreprocessor(\n",
    "    lowercase=True,\n",
    "    remove_punctuation=True,\n",
    "    remove_digits=True,\n",
    "    remove_stopwords=True,\n",
    "    apply_lemmatization=True\n",
    ")\n",
    "\n",
    "# Preprocess sample text\n",
    "processed_text = preprocessor.fit_transform([sample_text])[0]\n",
    "\n",
    "print(\"\\nPROCESSED TEXT:\")\n",
    "print(\"-\" * 50)\n",
    "print(processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process all texts (small sample for demonstration)\n",
    "sample_df = df.sample(n=1000, random_state=42)  # Use smaller sample for notebook\n",
    "\n",
    "print(\"Processing text data...\")\n",
    "sample_df = sample_df.copy()\n",
    "sample_df['reviewText_processed'] = preprocessor.fit_transform(\n",
    "    sample_df['reviewText'].fillna('')\n",
    ")\n",
    "\n",
    "print(\"Text preprocessing completed!\")\n",
    "print(f\"Sample size: {len(sample_df)} reviews\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Training (Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train models on sample data\n",
    "trainer = SentimentModelTrainer(\n",
    "    random_state=42,\n",
    "    test_size=0.2,\n",
    "    cv_folds=3,  # Reduced for faster execution\n",
    "    use_smote=True\n",
    ")\n",
    "\n",
    "# Additional features\n",
    "additional_features = [\n",
    "    'helpful_votes', 'total_votes', 'helpfulness_ratio',\n",
    "    'review_length', 'review_word_count', 'summary_length',\n",
    "    'exclamation_count', 'question_count'\n",
    "]\n",
    "\n",
    "# Prepare data\n",
    "trainer.prepare_data(\n",
    "    sample_df, \n",
    "    text_column='reviewText_processed', \n",
    "    target_column='sentiment_binary',\n",
    "    additional_features=additional_features\n",
    ")\n",
    "\n",
    "print(\"Data prepared for training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train selected models (faster ones for notebook)\n",
    "models_to_train = ['logistic_regression', 'naive_bayes']\n",
    "\n",
    "results = {}\n",
    "for model_name in models_to_train:\n",
    "    print(f\"Training {model_name}...\")\n",
    "    result = trainer.train_model(model_name, perform_cv=True)\n",
    "    results[model_name] = result\n",
    "    print(f\"Test F1-Score: {result['test_f1']:.4f}\")\n",
    "\n",
    "print(\"\\nModel training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model comparison\n",
    "summary_df = trainer.get_results_summary()\n",
    "print(\"MODEL COMPARISON:\")\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models with detailed metrics\n",
    "evaluator = ModelEvaluator(save_plots=False)\n",
    "\n",
    "evaluation_results = {}\n",
    "\n",
    "for model_name in models_to_train:\n",
    "    if model_name in trainer.models:\n",
    "        model = trainer.models[model_name]\n",
    "        \n",
    "        # Get predictions\n",
    "        y_test_pred = model.predict(trainer.X_test_text)\n",
    "        \n",
    "        # Get probabilities\n",
    "        try:\n",
    "            y_test_proba = model.predict_proba(trainer.X_test_text)[:, 1]\n",
    "        except:\n",
    "            y_test_proba = None\n",
    "        \n",
    "        # Evaluate\n",
    "        eval_result = evaluator.evaluate_single_model(\n",
    "            trainer.y_test, y_test_pred, y_test_proba, model_name\n",
    "        )\n",
    "        \n",
    "        evaluation_results[model_name] = eval_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "if evaluation_results:\n",
    "    comparison_df = evaluator.compare_models(evaluation_results)\n",
    "    \n",
    "    # Plot metrics comparison\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    \n",
    "    metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
    "    x = np.arange(len(comparison_df))\n",
    "    width = 0.15\n",
    "    \n",
    "    for i, metric in enumerate(metrics):\n",
    "        if metric in comparison_df.columns:\n",
    "            ax.bar(x + i*width, comparison_df[metric], width, label=metric)\n",
    "    \n",
    "    ax.set_xlabel('Models')\n",
    "    ax.set_ylabel('Score')\n",
    "    ax.set_title('Model Performance Comparison')\n",
    "    ax.set_xticks(x + width * 1.5)\n",
    "    ax.set_xticklabels(comparison_df['Model'])\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix for best model\n",
    "if trainer.best_model:\n",
    "    best_model_name = trainer.best_model['name']\n",
    "    best_model = trainer.best_model['model']\n",
    "    \n",
    "    y_pred = best_model.predict(trainer.X_test_text)\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    evaluator.plot_confusion_matrix(\n",
    "        evaluation_results[best_model_name]['confusion_matrix'],\n",
    "        class_names=['Negative', 'Positive'],\n",
    "        model_name=best_model_name\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Model Prediction Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test predictions on new text\n",
    "test_reviews = [\n",
    "    \"This guitar is amazing! Great sound quality and very well built. Highly recommended!\",\n",
    "    \"Terrible product. Broke after one week. Complete waste of money.\",\n",
    "    \"It's okay, not bad but not great either. Does the job.\"\n",
    "]\n",
    "\n",
    "if trainer.best_model:\n",
    "    # Preprocess test reviews\n",
    "    test_reviews_processed = preprocessor.transform(test_reviews)\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = trainer.predict(test_reviews_processed)\n",
    "    probabilities = trainer.best_model['model'].predict_proba(test_reviews_processed)\n",
    "    \n",
    "    print(\"PREDICTION EXAMPLES:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for i, (review, pred, prob) in enumerate(zip(test_reviews, predictions, probabilities)):\n",
    "        sentiment = \"Positive\" if pred == 1 else \"Negative\"\n",
    "        confidence = max(prob)\n",
    "        \n",
    "        print(f\"Review {i+1}: {review[:50]}...\")\n",
    "        print(f\"Prediction: {sentiment} (Confidence: {confidence:.3f})\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SENTIMENT ANALYSIS PIPELINE SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Dataset size: {len(sample_df)} reviews (sample)\")\n",
    "print(f\"Models trained: {len(results)}\")\n",
    "\n",
    "if trainer.best_model:\n",
    "    best_name = trainer.best_model['name']\n",
    "    best_score = trainer.best_model['score']\n",
    "    print(f\"Best model: {best_name} (F1-Score: {best_score:.4f})\")\n",
    "\n",
    "print(\"\\nNext Steps:\")\n",
    "print(\"- Run the complete pipeline with main.py for full dataset\")\n",
    "print(\"- Experiment with hyperparameter tuning\")\n",
    "print(\"- Try advanced models (XGBoost, Neural Networks)\")\n",
    "print(\"- Deploy the best model for production use\")\n",
    "print(\"- Collect more recent data for model updates\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}